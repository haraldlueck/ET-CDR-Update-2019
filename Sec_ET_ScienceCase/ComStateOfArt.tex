%\emph{
%Author(s):  S. Aoudia, L. Bosi, L. Storchi,  \\
%}
\dots
\FloatBarrier
\subsection{Computing: Status of Art and evolutive processes}

Computing has made great strides in recent years in order to provide each year even more fast processors.
Since the invention of the integrated circuit in 1958, the number of transistors on a given chip double 
(roughly) every two years, as states the first Moore's law \cite{MooreLaw}. This exponential growth has allowed computers to get 
both cheaper and more powerful at the same time. In order to mantain this trend for the next decade the next technological steps need some rethinking of basic foundations such as:
\begin{itemize}
\item Process technology
\item Hardware Architecture
\item Software % <==============((((     to check 
\end{itemize}

In this section we report about processors, computing infrastructure and architectures status of art, highligthing technical and physical  problems that are leading major vendors to change Hardware and Software architectural paradigma.  
\FloatBarrier
\subsubsection{Moore's law}

``Moore's Law is a violation of Murphy's Law. Everything gets better and better''~\cite{MooresFrase2}
%("Moore's Law at 40 . Happy birthday". The Economist. 2005-03-23. http://economist.com/displaystory.cfm?storyi\_id=3798505. Retrieved 2006-06-24)
 this is how Gordon Moore
commented the law, that bears his name, in 2005. Moore's law describes trend in the history of computing
hardware. The law has been originally thought to describes the number of transistors that can be placed
inexpensively on an integrated circuit. But now we see this law can be applied also to the capabilities
of many digital electronic devices, such as memory capacity, sensors and even even the number and size  %<===(( check 
of pixels in digital cameras. There are in fact many other laws related to the Moore's one. Other laws
prediction for example hard disk storage cost per unit of information, or network capacity, or pixels
per dollar, and more.

Gordon E. Moore formulated the law by a simple observation. In 1965 he noted that number of components
in integrated circuits had doubled every two years from the invention of the integrated circuit in 1958. Thus, he predicted that the trend would continue ``for at least ten years''. Years after the law has been
reformulated to take into account an higher growth, and the final formulation state that integrated circuits
would double in performance every 18 months. Thus ``Moore's first law predict'' an exponential rates for the transistor counts in a microprocessor: $P_n = P_0 \cdot 2^n$, where $P_n$ is the predicted computer processing power in
future years, $P_0$ is the actual computer processing power, and $n$ the number of years divided by the doubling period, expressed in years. I.e.\ if we consider the transistor count the doubling factor is 2 (every 2 years), while if we consider the processors speed the doubling factor is 1.5 (every 18 months).

Moore's first law can be viewed just as an observation, but maybe there is even more. Maybe behind
it there is a more deeper law, a law driving evolution of information and technology, of which the
Moore's law is just a consequence. But up to now what it is clear is that this law has been
widely accepted, and is used as a goal for both marketing and engineering departments of semiconductor
manufacturers. %We recall what has been reported in the previous section about top500 trend, that confirms the Moore's law.

\FloatBarrier
\subsubsection{Main difficulites on computing power increase}

Quoting Mr Moore recent statment  issued during an interview "..by about 2020, my law would come up against a rather 
intractable stumbling block: the laws of physics.."~\cite{MooresFrase1}  we may be led to think that the Moore's Law is at the end.
Today the integration scale (the typical size for the CMOS realization) is about 32-22nm that is comparable to few hundreds of atomic radii. It is so clear that one of the main limitation on continuing the actual processors renewal 
strategies is posed by the atomic limit. The difficulty on even more reducing the integration scale was evident and concrete already during the last 10 years. In fact major manufacturers introduced several technological 
innovations and hardware paradigma in order to provide even more fast processor, limiting the  integration scale reduction  and CPU frequencies increase. 

We report on Computing power increase issue, introducing briefly the top500 and the last 10 years technological innovations.
% (Loriano) 
% Report TOP500 and Intel and AMD data
% Energy consumption and Frequency increase

%scritto al volo da rivedere completamente, aggiungere dati su consumi e top dei
%consumi.

{\bf The Top500} project~\cite{website:top500} goal is to generate, twice a year, a list of the 500 most powerful
computers in the world. The list is manteined by Hans Meuer 
of the University of Mannheim, Erich Strohmaier and Horst Simon of Lawrence Berkeley National 
Laboratory, and Jack Dongarra of the University of Tennessee. Dongarra invented
Linpack benchmark~\cite{linpackb} 
many years ago, used to rank the system within the list. 

The Top 500 ranking has always been a good overview of the actual technology
trends, and along the last ten/twenty years the list has showed an evident trends toward parallel and
massively parallel systems. This trend has been confirmed by the appearance in the home computing system of superscalar and pipilened 
CPU firstly, and multi-core CPU after. 

%Looking on this list, it is clear that during last 20 years the performance gain has been mainly achieved through increases in system scale. For example an approach has been to use a  larger number of components and not to improve the single-processor performance, because of the cited limitation 
%on miniaturization and frequency growing. 

The actual top500 list emphasizes the following evolutive characteristics: 
\begin{itemize}
\item Natural evolution from multi-core to many-core era
\item Many-core architecture (i.e.\ GPU) together with multi-core X64 processors are in current use
\item Moore's Law is alive and still works well
\item Development of faster and more integrated interconnects as an obvious consequence of the increase in number nodes and cores 
\end{itemize}
% is again an obvious consequence of computing power request.

To stress those aspects it is essentials to cite the first ranked system on November 2010: the Tianhe-1A. This 
supercomputer is stationed at the National Supercomputer Center in 
Tianjin, it has achieved a performance level of 2.57 petaflop/s. The system collects together both the trends, fast interconnection and
many-core, It is clear that a new programming paradigms and new algorithms are needed to get top performance  from these computing infrastructure.


%%%%%%%%   box Tianhe-1A

\etbox{i}{Tianhe1Abox}{Tianhe-1A}{
This system equipped with 29376 GB of memory, 
is based on Nvidia graphics processing units (GPUs) as well as Intel Xeon 5600 series processors.
The Tianhe-1A is able to handle data at about twice the speed of InfiniBand, and the system 
has achieved a performance level of 2.57 petaflop/s essentially thanks to the the acceleration 
given by Nvidia Fermi GPU processors.
}

%%%%%%%% end box Tianhe-1A


%This aspect is stressed by two numbers, Tianhe-1A sustained performances: 2.57 
%petaflop/s, and peak performances: 4.70 petaflop/s. Such a difference   

Looking for high performances, one has also to deal with {\bf power consumption}. The power consumption of an HPC resource is a fundamental aspect on computing facilities setup. This requires an optimization  in terms of efficiency, cost and resources reliability. In order to stress how this is today an extremely important and sensible theme, the Top500 has started to collect data related to power consumption of the 500 most powerful computer in the world.

To achieve greater performance per compute node, vendors have increased not just 
transistors and speed but consequently also the power density. Thus, a large-scale 
high-performance computing system requires continual cooling in a 
large machine room, or even a new building in order to work properly. So to achieve greater performances one has  
to consider direct and indirect (i.e.\ cooling system) costs. We have to remember the HPC systems failure rate  increase is  directly related to the working temperature. 

%This second aspect is particularly important both because of cost, but mostly because 
%of its role in the software side. The software needs to be written in order to deal 
%with the intrinsic unreliability of the computing systems. So needs to be developed 
%following a different "paradigm", that is for example transparent run-time migration of tasks 
%between different computing resource, automatic management of recovery points, etc. etc.

Many-core seems to provide good performances also in terms of power consumption. In fact making the assumtion that The Tianhe-1A 2.5 PFlops system was built entirely with CPUs, it would consume more 
than {\bf 12 MW}. Thanks to the use of GPUs in a heterogeneous computing environment, Tianhe-1A consumes only {\bf 4 MW}, making it {\bf 3 times more power efficient}. %The most energy efficient supercomputers are based on: IBM BlueGene/Q Prototype with 1680 Mflop/watt, Fujitsu K-Computer at Riken with 829 Mflop/watt, and QPace Clusters based on IBM PowerXCell 8i processor, up to 774 Mflop/watt. Both the BlueGene/Q and the PowerXCell 8i are example of many-core systems.  

It is importaint to compare the power consumption of Tianhe-1A and Jaguar Cray XT5-HE, that is the second ranked system in top 500. In fact the
Jaguar use 5-10 MW to achieve 1.7 PFlops while Tianhe-1A use 4 MW to achieve 2.5 PFlops. {\bf This is a success of the many-core architecture providing 3-4 time more computing power per Watt!} 

This enforce the evidence that computing hardware architecture are moving toward the many-core era direction.
\FloatBarrier
\subsubsection{20 years of Parallelization}

In this section we introduce more details about architecture innovation introduced along last 20 years by hardware manufacturers. We intend to underline as the implicit and explicit parallelization concept has been used as a way to go around the miniaturization limitations and frequency increase.

%We will briefly recall the various steps that historically brings the first scalar CPU to the modern many-core architecture.

A scalar processor is the simplest CPU that can considered. Is is capable 
of executing a single instruction per clock cycle and to manipulate one or two data items at a time. 

A superscalar processor is instead capable of intrinsic parallelism. Each instruction processes one data item, but multiple instructions and data are  processed concurrently, having multiple redundant functional units within each CPU. In fact modern superscalar processors includes multiple 
ALUs, multiple FPUs. Thus the dispatcher of the CPU reads instructions from memory and decides which ones can be run in parallel. An important step
forward has been the introduction (around 1998/1999)  of one or more {\bf SIMD} units by AMD and Intel. Those units are used through the AMD's 3DNow and the Intel's Streaming SIMD Extensions (SSE) instruction set extension  to perform
basic vector operations (i.e.\ adding two vectors of float, in one step).


The capability of executing more than one instruction per clock cycle is another level of parallelism introduced into superscalar CPU. The basic idea is to split each instruction into several micro-instructions, each executed by an indipendent functional unit of the pipeline.

This approach permits a natural parallelism. In fact usually  when there are
several instructions to be executed, as soon as the first functional unit 
has finished the execution of the first micro-instruction, this is sent to 
the second unit. So the first functional unit of the pipeline is free to start the execution of the second instruction, and so on.
Given a starting latency to fill the pipeline, the CPU reach a steady state where N instructions are executed together for each clock cycle, where N is number of functional units (so called depth of the pipeline).


Another step on improving the efficiency of CPUs, has been the introduction of {\bf Simultaneous multithreading (SMT)}, roughly about 2003-2004. Maybe one of the most famous implementation of 
this technique is the Intel's Hyper-Threading Technology. The HT, or HTT, works duplicating some sections of the processor pipeline. In this way the hyper-threading processor appears as two "logical" processors to the host operating system. This allows the operating system to schedule two threads or processes simultaneously.     

Starting from 2005 multi-core CPU have been introduced in the everyday computing architecture, both in the embedded and standard systems. This solution implements multiprocessing in a single physical package, namely the full processor, replicating the whole computing core. Different cores may or not share caches, and may implement message passing or shared-memory inter-core communication. The actual multi-core CPU implements up to four/six cores 
per package. In case of the multi-core CPU the performance gain is strictly related to the efficiency
of the parallelized software. About that we have cite Amdahl's law\cite{Amdahl}, that connect the parallelization
efficiency with the fraction of the software that can be parallelized to run on multiple cores simultaneously.
\FloatBarrier
\subsubsection{Emerging technologies for distributed computing}

In the this section we want to briefly review some new and emerging
technologies like grid and cloud computing. We will try to briefly introduce
booth the technologies, and explain similarities, differences, and issues to 
consider in grid and cloud computing.

As the complexity of a computational problem grows, it is crucial, in
order to continue being able to solve it, to design a computational
system flexible enough to grow comparably in complexity. Solutions can
be worked out only by gathering sufficient computing power and
the appropriate know-how. 
Know-how and computing power must be collected and deployed
in an efficient
and appropriate manner, so that anyone is enabled to access
transparently and simply any needed information and resources. 
What we have just pointed out is the general idea of the grid or,
perhaps more precisely, of a computational grid environment. This new
approach to network computing is known by several names such as:
meta computing, scalable computing, global computing, internet computing,
and, more recently, peer-to-peer computing.

One of the most famous
definitions of the grid perfectly describes such design: "the grid is a
flexible, secure, coordinated resource sharing among dynamic collections
of individuals, institutions, and resources - what we refer to as
virtual organizations." foster at al.~\cite{anatomy_grid}. 
more precisely the grid can be thought as a distributed system, where 
heterogeneous resources are geographically dispersed and connected by a network.
So grids represent a form of distributed computing facilities, where a 
"super virtual computer" is composed by many interconnected computing resources,
like clusters, single workstations and traditional single super computers.
The middleware (i.e.\ a collection of software libraries), like the operating system in a pc, gives to the user all
the necessary instruments to use the grid in a transparent and secure
way, it provides uniformity through a standard set of interfaces to the
underlying resources. It is a layer of software between the network and
the applications that provides services such as identification,
authentication, authorization, directories, and security. 

A working example of what we briefly described is the Worldwide LHC Computing Grid project (WLCG)~\cite{lcgwebsite}
currently operates the world's largest scientific Grid, with over 
140 computing centers in 34 countries contributing
resources, including more than 10,000 CPUs and several Petabytes of storage. 

%%%%%%%%   box WLCG

\etbox{i}{Gridbox}{Worldwide LHC Computing Grid (WLCG)}{WLCG is the computational solution adopted by the LHC to store and analyze the expected Petabytes per year of data generated by the 
four main LHC experiments (ALICE, ATLAS, CMS and LHCb). It consists of a hierarchical network of computational centers distributed 
in 33 countries.  The zeroth level (TIER0) is the CERN Computer Centre. It is responsible for the safe-keeping of the raw data 
and its distribution to the first level sites (TIER1). These are the principal centers dedicated to store and analyze all the data 
uploaded by the LHC detector. They are hosted by  : Canada, France, Germany, Italy, Netherlands, Nordic countries, Spain, Taiwan, 
United Kingdom, USA. The second level sites (TIER2), which are more numerous  but less important, store temporarily a subset of 
data loaded from the TIER1 sites. In addition to these computational centers, each collaborating institution could have its own 
cluster (TIER3) with regulated access \cite{AdvanceComputing, TIERS}.  
WLCG achieved in 2011 the following capacities 
\cite{Capacities}
: 
\begin{itemize}
\item 87,089 physical CPUs, 
\item 280,510 logical  CPUs,
\item 158,526,879 GB of total online storage capacity,
\item 104,588,138 GB of total near-line storage capacity,
\item more than 2,000 TFlops of processing power.
\end{itemize}
}

%%%%%%%% end box WLCG



Cloud computing goes a step further in the direction of separating the user from the 
computing resources. This new computing paradigm represents also an improvement in the direction 
of the on-demand resource provisioning. Cloud computing generally means a collection of technologies 
enabling the final user to benefit of wide set of hardware and or software remotely distributed.
We can compares the Cloud with the electric power
network: when we switch on a light, or we plug an
electrical device into a wall socket, we are not aware from where the
power comes from, and generally we do not care much about such details.
Now we are thinking about a world in which computer power, storage and software capabilities 
are as easily accessible as the electric power. 

Wen can try to briefly summarize the Cloud computing architecture as follow. 
The final user simply uses a specific service provided by a customer administrator.
The administrator uses some interfaces to select a specific service (i.e.\ a virtual server or 
just some storage) and to administer the service (i.e.\ configure it, activate or deactivate 
the service, or maybe to ask for more computing power or storage). 
The service provider is the one that physically owns the real server, and 
it is the one in charge for providing some transparent interface to manage 
the resources.

There are up to now several examples of working cloud infrastructures, here we report just a 
short list:

\begin{itemize}
\item Amazon Elastic Compute Cloud (EC2), that allows users to rent and use virtual server  
for the scalable deployment of applications.
\item Amazon S3 (Simple Storage Service), an online storage web service
\item Google App Engine (GAE), a platform for developing and hosting web applications 
in Google-managed data centers

\end{itemize}


%%%%%%%%%% Box E@H
\etbox{i}{EatHbox}{Einstein@Home}{Einstein@Home \cite{EatH} 
is a volunteer distributed computing program that uses more than half a million personal computer's 
idle\footnote{A computer processor is described as idle when it is not being used by any program.} time (400 TFlops \cite{Server}) 
to search for gravitational waves from  isolated pulsars or radio pulsars in binaries using data from the LIGO gravitational 
wave detector and the Arecibo Observatory in Puerto Rico \cite{Puerto}. The project is hosted by the University of Wisconsin-Milwaukee 
(USA) and the Max Planck Institute for Gravitational Physics (Albert Einstein Institute, Hanover, Germany).}
%%%%%%%%%%% end Box E@H

\subsubsection{Manycore}

The transition to many-cores systems seems to be the natural evolution of computational architecture. Many-core processoris have a larger number of cores respect to traditional multi-processor, roughly in the range of several tens of cores. The actual state of art in many-core architecture is represented by GPU processors, where in
a single package hundreds of computing cores are implemented. 
%In any case the trend is toward CPU with hundreds of cores, or
%multi-core chips mixed with simultaneous multithreading, memory-on-chip, and special-purpose "heterogeneous" cores,
%as the Top500 shows.

%appunto 1: The article CPU designers debate multi-core future by Rick Merritt, EE Times 2008, includes comments:
%"Chuck Moore [...] suggested computers should be more like cellphones, using a variety of specialty cores to 
%run modular software scheduled by a high-level applications programming interface.
%[...] Atsushi Hasegawa, a senior chief engineer at Renesas, generally agreed. He suggested the cellphone's use 
%of many specialty cores working in concert is a good model for future multi-core designs.
%[...] Anant Agarwal, founder and chief executive of startup Tilera, took the opposing view. He said multi-core 
%chips need to be homogeneous collections of general-purpose cores to keep the software model simple."

%appunto 2: A many-core processor is one in which the number of cores is large enough that traditional 
%multi-processor techniques are no longer efficient. We need a network on chip.

In the next part of the section, we would like to provide some information about major vendors technological decisions and 
roadmaps in order to give the feeling about many-core technological trends.

  
All major CPU processors manufactures, such as Intel and AMD, are researching and developing new innovative solutions in
 order to  bypass the even more stringent technological challenges. In our work we have collected several information 
about major manufacturer production roadmaps and production planning.

As been previously reported in some sense GPU  are the precursor of many-core architecture with several already marketed 
and used hardware devices. Even if these have been developed specifically for computer graphics this hardware is now 
 widely used in other computing fields, proving a resounding success.

Moreover during 2010, Intel and AMD have published and made official communication about subsequent CPU 
generation. Also if in different way and implementation they report technological solution that are following the path of increasing number of computing core elements per CPU.

In detail AMD has declared to be close to release a new processors family  based on Fusion~\cite{amdfusion}. AMD Fusion is the 
codename for next-generation microprocessor design and a product merging together AMD and ATI. Where AMD brings knowledge about CPU technology and ATI its own knowledge about Graphic Progessing Units, combining 
general processor execution as well as 3D geometry processing and other functions of modern GPUs into a single package. The core of this new architecture are the APU (Accelerated Processing Units).
This technology is expected to debut in the first half of 2011. 

Intel has recently declared during the New Orleans Supercomputing Conference its approach to the High Performance Computing, 
introducing the MIC (Many Integrated Core) solution~\cite{intelmic}, known with the name Knights Ferry. The Intel MIC architecture is derived from several Intel
projects, including "Larrabee"~\cite{larrabee} and such Intel Labs research projects as the Single-chip Cloud Computer~\cite{intelscc,intelsccp}. 
The architecture is based on chip containing 
32 cores x86 at 22nm. Moreover Intel has declared for 2012 the production of an higher solution based on 50 core 4 hyper-threading 
processor. Obviously this solution can be compared directly with GPGPU, having  an equivalent high number of cores. 
One of the key point of the Intel solution is the code portability, being a x86 compatible architecture. Moreover comparing Knights Ferry 
 with GPU solution we have to remark that a CPU core is much more complex than a GPU core, providing for example SSE4, permitting 8 single precision operations per cycle per core. 

In this context it is interesting to report also other experiences, such as Tilera products~\cite{Tilera}. It provided the first innovative many-core solution based on x86 architecture. For example 
the 64-core TILEPro64 integrates 64  identical cores, having a complete full-featured processor, 
including cache memories and more.

The previous  statements indicate a clear direction about new processor products: {\bf CPU are evolving toward the direction of the many-core computing}. Each vendor is traducing this concept on different shapes (i.e.\ ``homogeneous collections of general-purpose cores'' rather than ``computers using a variety of specialty cores''), 
but all agree on the need of increase significantly the parallelization
level~\cite{intelterascale}.

A so deep changes in the hardware architecture will require also a deep changes on software side and about 
the way of thinking algorithms. Without this effort it is impossible to extract the real power of these new computing resources.

%(+references we can cite intel from here:
%http:\/\/techresearch.intel.com\/newsdetail.aspx?Id=17\#SCC
%for AMD instead we can start from here: 
%http:\/\/www.amd.com\/us\/aboutamd\/research\/radl\/Pages\/news.aspx)

\subsubsection{Manycore as real scenario for future computing infrastructure}\label{sec:manycor}

In order to understand the real capabilities of these new architectures, in Perugia group we started to explore the status of art of manycore devices. In particular we perform some test with NVIDIA C1060 (i.e.\ GT200) and with NVIDIA C2050 (i.e.\ the brand new Fermi GPU). 

Our tests are based on a fully Multi-GPU implementation of a Coalescing Binaries Detection pipeline. This software includes specifically an input data conditioning, signal Post Newtonian generator up to PN 3.5~\cite{PGTesiPN} and a complete matched fitlering procedure with colored noise~\cite{GPUarxive}.

Respect to CPU implementation  of the same algorithms, results show an average gain factor (normalized by price) of about 50, using a single C1060
GPU. This can be translated in a number of applyied matched filtering per seconds of about {\bf 30}. Obviously this number depends on vector size. This numbers are about length of $2^23$ samples. Using shorter vector it is possible to achieve higher gains.  Performing the same test with new NVIDIA Fermi GPU (i.e.\ the Tesla C2050), 
this number increase up to {\bf 120 templates per second}. 

In Perugia we implemented also a Multi-GPU version of the pipeline, which gives another increasing factor of 3.5 using 4 GPU, bringing us to an
impressive result of about {\bf 400 templates per second} processed~\cite{PGTesiMULTI}. 

Another interesting gain factor is about FFT algorithm. Several benchmarks reports a gain factor of  50-80 using GPU respect to CPU architecture. This value has been renormalized by device prices.

Thus, we can state that using the already available many-core thecnologies the gain factor respect to the standard single core architecture is, conservatively speaking, about 100. Obviously an exhaustive analysis of the gain factor needs to deal also with power consumption and costs.   




\FloatBarrier
% StateOfArt
%===== nomenclature ================================================
% \nomenclature[]{}{ }
%
%CMOS realistaion

